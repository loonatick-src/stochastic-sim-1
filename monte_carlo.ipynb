{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit, vectorize, float64, prange\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "from os.path import exists\n",
    "from scipy import stats\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 180\n",
    "mpl.rcParams[\"legend.fontsize\"] = 11\n",
    "mpl.rcParams[\"font.size\"] = 11\n",
    "plt.rc(\"text\", usetex=False)\n",
    "plt.rc(\"text.latex\", preamble=r\"\"\"\n",
    "\\usepackage{palatino}\n",
    "\\usepackage{newpxmath}\"\"\")\n",
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_intervals(sample_variances, p):\n",
    "    \"\"\"\n",
    "    Returns the 100(1-p)% confidence intervals\n",
    "    sample_variances    ----------\n",
    "    sample_variances: array\n",
    "        array of sample variances\n",
    "    \n",
    "    p: float between 0 and 1 exclusive\n",
    "        p-value for confidence interval\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    intervals: array of floats\n",
    "        |x - μ| < intervals, where X is the sample mean\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Calculated as\n",
    "        `stats.t.ppf(q = 1 - p/2, df=n-1) * np.sqrt(sample_variances)/np.sqrt(n)`\n",
    "    where n = `len(sample_variances)`\n",
    "    \"\"\"\n",
    "    n = len(sample_variances)\n",
    "    intervals = stats.t.ppf(q = 1 - p/2, df=n-1) * np.sqrt(sample_variances)/np.sqrt(n)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def random_uniform(low = 0.0, high = 1.0, size = 1, seed = 0):\n",
    "    if (seed):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    randos = np.random.random(size = size)\n",
    "    scale = np.abs(high - low)\n",
    "    randos *= scale\n",
    "    randos += low\n",
    "    return randos\n",
    "\n",
    "@njit\n",
    "def random_uniform_orthogonal(nax, l_bounds, u_bounds, seed = 0):\n",
    "    \"\"\"\n",
    "    TODO: license\n",
    "    TODO: docstring\n",
    "    TODO: seeding?\n",
    "    \"\"\"\n",
    "    if (seed):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    N = np.power(nax, 2)\n",
    "    x_scale = (u_bounds[0] - l_bounds[0])/N\n",
    "    y_scale = (u_bounds[1] - l_bounds[1])/N\n",
    "    x0 = l_bounds[0]\n",
    "    y0 = l_bounds[1]\n",
    "    xlist = np.arange(N).reshape((nax,nax))\n",
    "    ylist = np.arange(N).reshape((nax,nax))\n",
    "    x_randos = np.empty(N, dtype = np.float64)\n",
    "    y_randos = np.empty(N, dtype = np.float64)\n",
    "    for i in range(nax):\n",
    "        xlist[i] = np.random.permutation(xlist[i])\n",
    "        ylist[i] = np.random.permutation(ylist[i])\n",
    "    for i in range(nax):\n",
    "        for j in range(nax):\n",
    "            x_randos[i*nax + j] = x0 + x_scale * (xlist[i][j] + (np.random.random()))\n",
    "            y_randos[i*nax + j] = y0 + y_scale * (ylist[j][i] + (np.random.random()))\n",
    "    return x_randos, y_randos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython = True)\n",
    "def default_escape_cond(z):\n",
    "    return np.abs(z) >= 2\n",
    "\n",
    "@jit(nopython = True)\n",
    "def main_cardioid(arg):\n",
    "    point = 0.5 * np.exp(1j*arg) - 0.25 * np.exp(2*1j*arg)\n",
    "    return np.abs(point)\n",
    "\n",
    "#@jit(nopython = True)\n",
    "#def cardioid_escape_cond(z):\n",
    "    #if np.abs(z) <= main_cardioid(z.)\n",
    "\n",
    "def mandelbrot_batch_factory(escape_condition):\n",
    "    @jit(nopython = True, parallel = True, nogil = True)\n",
    "    def _mandelbrot_mc(max_iter, samples):\n",
    "        iter_counts = np.empty(len(samples), dtype = np.int64)\n",
    "        for i in prange(len(samples)):\n",
    "            c = samples[i]\n",
    "            zn = c\n",
    "            for count in range(max_iter):\n",
    "                zn = zn * zn + c\n",
    "                if (escape_condition(zn)):\n",
    "                    iter_counts[i] = count\n",
    "                    break\n",
    "                elif count == max_iter - 1:\n",
    "                    iter_counts[i] = count\n",
    "        return iter_counts\n",
    "    return _mandelbrot_mc\n",
    "\n",
    "mandelbrot_mc_default = mandelbrot_batch_factory(default_escape_cond)\n",
    "\n",
    "@njit\n",
    "def create_batch_uniform(N, re_low, re_high, im_low, im_high, seed = 0):\n",
    "    if (seed):\n",
    "        np.random.seed(seed)\n",
    "    samples_re = random_uniform(low = re_low, high = re_high, size = N)\n",
    "    samples_im = random_uniform(low = im_low, high = im_high, size = N)\n",
    "    samples = samples_re + 1j*samples_im\n",
    "    return samples\n",
    "\n",
    "@njit\n",
    "def create_batch_orthogonal(N, re_low, re_high, im_low, im_high, seed = 0):\n",
    "    if (seed):\n",
    "        np.random.seed(seed)\n",
    "    nax = np.int64(np.ceil(np.sqrt(N)))\n",
    "    l_bounds = (re_low, im_low)\n",
    "    u_bounds = (re_high, im_high)\n",
    "    samples_re, samples_im = random_uniform_orthogonal(nax, l_bounds, u_bounds)\n",
    "    samples = samples_re + 1j*samples_im\n",
    "    samples = samples[:N]\n",
    "    return samples\n",
    "\n",
    "def create_batch_lhc(N, re_low, re_high, im_low, im_high, seed = 0):\n",
    "    if (seed):\n",
    "        sampler = stats.qmc.LatinHypercube(d = 2, seed = seed)\n",
    "    else:\n",
    "        sampler = stats.qmc.LatinHypercube(d = 2)\n",
    "    re_scale = re_high - re_low\n",
    "    im_scale = im_high - im_low\n",
    "    samples = sampler.random(N)\n",
    "    samples[:,0] *= re_scale\n",
    "    samples[:,1] *= im_scale\n",
    "    samples[:,0] += re_low\n",
    "    samples[:,1] += im_low\n",
    "    samples = samples[:,0] + 1j*samples[:,1]\n",
    "    return samples\n",
    "\n",
    "@njit\n",
    "def genrand_uniform(low = 0.0, high = 1.0, size = 1, sampler = random_uniform):\n",
    "    return sampler(low, high, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython = True)\n",
    "def set_count_from_iter_counts(max_iter, iter_counts):\n",
    "    set_count = 0\n",
    "    for count in iter_counts:\n",
    "        if count == max_iter - 1:\n",
    "            set_count += 1\n",
    "    return set_count\n",
    "\n",
    "@jit(nopython = True)\n",
    "def area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, max_iter):\n",
    "    n = len(iter_counts)\n",
    "    set_count = set_count_from_iter_counts(max_iter, iter_counts)\n",
    "    domain_area = (re_high - re_low) * (im_high - im_low)\n",
    "    return domain_area * set_count / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython = True, parallel = True, nogil = True)\n",
    "def mandelbrot_mc(max_iter, samples):\n",
    "    \"\"\"\n",
    "    Counts the number of complex numbers in `samples` that do\n",
    "    not meet the escape condition in `max_iter` recursions of\n",
    "    the Mandelbrot polynomial, z(n+1) = z(n)^2 + c\n",
    "    \n",
    "    The escape condition used is |z(n)| >= 2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_iter: positive integer\n",
    "        Value of n up to which z(n) is evaluated\n",
    "    \n",
    "    samples: array of complex numbers\n",
    "        values of z(0)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    set_count: positive integer\n",
    "        Number of complex numbers in `samples` that remain\n",
    "        that do not satisfy the escape condition\n",
    "    \"\"\"\n",
    "    set_count = 0\n",
    "    for i in prange(len(samples)):\n",
    "        c = samples[i]\n",
    "        zn = c\n",
    "        for j in range(1, max_iter):\n",
    "            zn = zn*zn + c\n",
    "            if (np.abs(zn) > 2):\n",
    "                # numba should recognize this as a critical section\n",
    "                break\n",
    "            elif j == max_iter - 1:\n",
    "                set_count += 1\n",
    "    return set_count\n",
    "\n",
    "@jit(nopython = True, nogil = True, parallel = True)\n",
    "def mandelbrot_mc_area(re_low, re_high, im_low, im_high, max_iter, samples):\n",
    "    \"\"\"\n",
    "    Calculates estimator of the area of the Mandelbrot set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    re_low, re_high, im_low, im_high: float or float-like\n",
    "        corners of bounding rectangle in the complex plane\n",
    "    \n",
    "    max_iter: positive integer\n",
    "        value of n up to which z(n) is evaluated\n",
    "    \n",
    "    samples: array of complex numbers\n",
    "        list of c values, assumed to be drawn from a uniform distribution\n",
    "        in the bounding region\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    area: default numpy float (float32 or float64)\n",
    "        estimate of the area of the mandelbrot set\n",
    "    \"\"\"\n",
    "    count = mandelbrot_mc(max_iter, samples)\n",
    "    return mandelbrot_area(count, len(samples), re_low, re_high, im_low, im_high)\n",
    "\n",
    "@jit(nopython = True)\n",
    "def mandelbrot_area(count, N, re_low, re_high, im_low, im_high):\n",
    "    \"\"\"\n",
    "    TODO: Add docstring\n",
    "    \"\"\"\n",
    "    rect_area = (re_high - re_low) * (im_high - im_low)\n",
    "    return rect_area * count / N\n",
    "\n",
    "@jit(nopython = True)\n",
    "def mandelbrot_mc_runs(max_iter, samples, runs_count):\n",
    "    counts = np.zeros(runs_count)\n",
    "    for _ in range(runs_count):\n",
    "        counts[i] = mandelbrot_mc(max_iter, samples)\n",
    "    return counts\n",
    "\n",
    "@jit(nopython = True)\n",
    "def sample_mean_variance(counts):\n",
    "    \"\"\"Returns sample mean and sample variance of input array\n",
    "    Parameters\n",
    "    ----------\n",
    "    counts: array of numbers\n",
    "    \n",
    "    Returns: 2-tuple (sample mean, sample variance)\n",
    "    \"\"\"\n",
    "    n = len(counts)\n",
    "    assert n > 1\n",
    "    sample_mean = np.mean(counts)\n",
    "    sample_variance = np.sum(np.power(counts - sample_mean, 2))/(n - 1)\n",
    "    return (sample_mean, sample_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "i = 1000\n",
    "re_low, re_high = -2., 1.\n",
    "im_low, im_high = 0, 1.5\n",
    "area = (re_high - re_low) * (im_high - im_low)\n",
    "samples = create_batch_uniform(N, re_low, re_high, im_low, im_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.83 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = timer()\n",
    "count = mandelbrot_mc_default(i, samples)\n",
    "end_time = timer()\n",
    "print(f\"Execution time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [WARN]: On Linux Mint 20, 16GB RAM, 2GB swap space, the kernel crashes as it runs out of memory for N = 1.0e9\n",
    "# N = 10**8\n",
    "# # TODO: save large random arrays\n",
    "# np.random.seed(10010)\n",
    "# samples = create_batch_uniform(N, re_low, re_high, im_low, im_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = timer()\n",
    "# count = mandelbrot_mc_default(i,samples)\n",
    "# end_time = timer()\n",
    "# print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-71845e16e962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batch_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_high\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0miter_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmandelbrot_mc_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mest_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marea_from_iter_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_high\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mareas_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest_area\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numba/core/serialize.py\u001b[0m in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_numba_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \"\"\"Used by `numba_unpickle` from _helperlib.c\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = 10**5\n",
    "i_values = np.array([10**i for i in range(2,8)], dtype = np.int64)\n",
    "\n",
    "re_low, re_high = -2., 1.\n",
    "im_low, im_high = 0, 1.5\n",
    "\n",
    "npz_filename = \"mc_fixed_s.npz\"\n",
    "\n",
    "mean_areas = np.empty(len(i_values))\n",
    "sample_variances = np.empty(len(i_values))\n",
    "\n",
    "if not exists(npz_filename):\n",
    "    number_of_batches = 50\n",
    "    for k,i in enumerate(i_values):\n",
    "        areas_batch = np.empty(number_of_batches)\n",
    "        for j in range(number_of_batches):\n",
    "            samples = create_batch_uniform(s, re_low, re_high, im_low, im_high)\n",
    "            iter_counts = mandelbrot_mc_default(i, samples)\n",
    "            est_area = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "            areas_batch[j] = est_area\n",
    "        μ, svar = sample_mean_variance(areas_batch)\n",
    "        mean_areas[k] = μ\n",
    "        sample_variances[k] = svar\n",
    "    np.savez(npz_filename, mean_areas, sample_variances)\n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    mean_areas = save_data['arr_0']\n",
    "    sample_variances = save_data['arr_1']\n",
    "    \n",
    "confidence_95 = confidence_intervals(sample_variances, p = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2500\n",
    "s_values = np.array([10**s for s in range(2,8)], dtype = np.int64)\n",
    "\n",
    "re_low, re_high = -2., 1.\n",
    "im_low, im_high = 0, 1.5\n",
    "\n",
    "npz_filename = \"mc_fixed_i.npz\"\n",
    "\n",
    "mean_areas = np.empty(len(s_values))\n",
    "sample_variances = np.empty(len(s_values))\n",
    "\n",
    "if not exists(npz_filename):\n",
    "    number_of_batches = 50\n",
    "    for k,s in enumerate(s_values):\n",
    "        areas_batch = np.empty(number_of_batches)\n",
    "        for j in range(number_of_batches):\n",
    "            samples = create_batch_uniform(s, re_low, re_high, im_low, im_high)\n",
    "            iter_counts = mandelbrot_mc_default(i, samples)\n",
    "            est_area = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "            areas_batch[j] = est_area\n",
    "        μ, svar = sample_mean_variance(areas_batch)\n",
    "        mean_areas[k] = μ\n",
    "        sample_variances[k] = svar\n",
    "    np.savez(npz_filename, mean_areas, sample_variances)\n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    mean_areas = save_data['arr_0']\n",
    "    sample_variances = save_data['arr_1']\n",
    "    \n",
    "confidence_95 = confidence_intervals(sample_variances, p = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed $i$, $\\lim_{s\\to\\infty}A_{i,s} = A_i > A_\\text{M}$, i.e. we are essentially calculating the area of a superset of the Mandelbrot set, one that includes points that would have met the escape condition after $i$ iterations.\n",
    "\n",
    "Given a strictly increasing sequence of $i$-values $\\{i_1, i_2, \\ldots i_n\\}$, we would like to pick $i = i_k$ for the smallest $k$ such that $|A_{k+1} - A_{k}|$ is less than some threshold value.\n",
    "\n",
    "Henceforth, we simulate only the top half of the mandelbrot set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_values = np.array([50, 100, 200, 500, 1000, 2000], dtype = np.int64) #TODO: tweak values\n",
    "s_values = np.array([10**i for i in range(4,9)], dtype = np.int64)\n",
    "if not exists(\"monte_carlo_random_sampling.npz\"):\n",
    "    re_low, re_high = -2., 1.\n",
    "    im_low, im_high = 0, 1.5\n",
    "\n",
    "    number_of_batches = 50\n",
    "    mean_areas = np.zeros((len(i_values), len(s_values)))\n",
    "    sample_variances = np.zeros((len(i_values), len(s_values)))\n",
    "\n",
    "    start_time = timer()\n",
    "    for k,i in enumerate(i_values):\n",
    "        # repeat `number_of_batches` times for each s-value\n",
    "        for l,s in enumerate(s_values):\n",
    "            areas_batch = np.zeros(number_of_batches)\n",
    "            for j in range(number_of_batches):\n",
    "                samples = create_batch_uniform(s, re_low, re_high, im_low, im_high)\n",
    "                iter_counts = mandelbrot_mc_default(i, samples)\n",
    "                est_area = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "                areas_batch[j] = est_area\n",
    "            μ, svar = sample_mean_variance(areas_batch)\n",
    "            mean_areas[k][l] = μ\n",
    "            sample_variances[k][l] = svar\n",
    "    end_time = timer()\n",
    "    np.savez(\"monte_carlo_random_sampling.npz\", mean_areas, sample_variances)\n",
    "else:\n",
    "    save_data = np.load(\"monte_carlo_random_sampling.npz\")\n",
    "    mean_areas = save_data['arr_0']\n",
    "    sample_variances = save_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((end_time - start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n number of batches\n",
    "\n",
    "def ttest_(means, variances, n):\n",
    "\n",
    "    critical_value = stats.t.ppf(q=0.975, df=n-1)\n",
    "    \n",
    "    t_test = (critical_value * np.sqrt(variances))/ np.sqrt(n)\n",
    "    \n",
    "    return t_test, critical_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_(means=mean_areas*2, variances=sample_variances*2, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variance_plotter_appendix(simulation_data, title):\n",
    "    \"\"\"\"\n",
    "    Plots mean area and standard deviation of the mandelbrot set \n",
    "    against sample sizes simulated with different number of \n",
    "    iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    s_values = np.array([10**i for i in range(4,9)], dtype = np.int64)\n",
    "    \n",
    "    data = np.load(simulation_data + \".npz\")\n",
    "    \n",
    "    mean_areas = data['arr_0']*2\n",
    "    \n",
    "    sample_variances = data['arr_1']*2\n",
    "    \n",
    "    mean_i_50 = mean_areas[0]\n",
    "    mean_i_100 = mean_areas[1]\n",
    "    mean_i_200 = mean_areas[2]\n",
    "    mean_i_500 = mean_areas[3]\n",
    "    mean_i_1000 = mean_areas[4]\n",
    "    mean_i_2000 = mean_areas[5]\n",
    "\n",
    "    std_i_50 = np.sqrt(sample_variances[0])\n",
    "    std_i_100 = np.sqrt(sample_variances[1])\n",
    "    std_i_200 = np.sqrt(sample_variances[2])\n",
    "    std_i_500 = np.sqrt(sample_variances[3])\n",
    "    std_i_1000 = np.sqrt(sample_variances[4])\n",
    "    std_i_2000 = np.sqrt(sample_variances[5])\n",
    "\n",
    "\n",
    "    plt.plot(s_values, mean_i_50, color = \"maroon\", alpha = 0.6, label = \"Approximation with 50 iterations\")\n",
    "    plt.fill_between(s_values, mean_i_50 - std_i_50, mean_i_50 + std_i_50, alpha = 0.3, color = \"maroon\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_100, color = \"firebrick\", alpha = 0.6, label = \"Approximation with 100 iterations\")\n",
    "    plt.fill_between(s_values, mean_i_100 - std_i_100, mean_i_100 + std_i_100, alpha = 0.3, color = \"firebrick\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_200, color = \"tomato\", alpha = 0.6, label = \"Approximation with 200 iterations\")\n",
    "    plt.fill_between(s_values, mean_i_200 - std_i_200, mean_i_200 + std_i_200, alpha = 0.3, color = \"tomato\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_1000, color = \"orange\", alpha = 0.6, label = \"Approximation with 1000 iterations\")\n",
    "    plt.fill_between(s_values, mean_i_1000 - std_i_1000, mean_i_1000 + std_i_1000, alpha = 0.3, color = \"orange\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_2000, color = \"yellow\", alpha = 0.6, label = \"Approximation with 2000 iterations\")\n",
    "    plt.fill_between(s_values, mean_i_2000 - std_i_2000, mean_i_2000 + std_i_2000, alpha = 0.3, color = \"yellow\")\n",
    "\n",
    "    #plt.hlines(1.50659, s_values[0], s_values[-1], label = \"Theoretical Mandelbrot area\", linestyles=\"dashed\")\n",
    "\n",
    "    plt.ylim(top=max(mean_i_50)+0.06) \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Mandelbrot area\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"upper right\", fontsize=\"x-small\")\n",
    "    \n",
    "    plt.savefig(title)\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_variance_plotter_appendix(simulation_data = \"monte_carlo_random_sampling\", title = \"line_plot_samples_random_appendix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_variance_plotter_appendix(simulation_data, title):\n",
    "    \"\"\"\"\n",
    "    Plots mean area and standard deviation of the mandelbrot set \n",
    "    against sample sizes simulated with different number of \n",
    "    iterations\n",
    "    \"\"\"\n",
    "    i_values = np.array([50, 100, 200, 500, 1000, 2000], dtype = np.int64) \n",
    "    \n",
    "    data = np.load(simulation_data + \".npz\")\n",
    "    \n",
    "    mean_areas = data['arr_0']*2\n",
    "    \n",
    "    sample_variances = data['arr_1']*2\n",
    "    \n",
    "    mean_s_10_4 = [i[0] for i in mean_areas]\n",
    "    mean_s_10_5 = [i[1] for i in mean_areas]\n",
    "    mean_s_10_6 = [i[2] for i in mean_areas]\n",
    "    mean_s_10_7 = [i[3] for i in mean_areas]\n",
    "    mean_s_10_8 = [i[4] for i in mean_areas]\n",
    "\n",
    "    std_s_10_4 = np.sqrt([i[0] for i in sample_variances])\n",
    "    std_s_10_5 = np.sqrt([i[1] for i in sample_variances])\n",
    "    std_s_10_6 = np.sqrt([i[2] for i in sample_variances])\n",
    "    std_s_10_7 = np.sqrt([i[3] for i in sample_variances])\n",
    "    std_s_10_8 = np.sqrt([i[4] for i in sample_variances])\n",
    "\n",
    "\n",
    "    plt.plot(i_values, mean_s_10_4, alpha = 0.5, label = \"Approximation with $10^4$ samples\")\n",
    "    plt.fill_between(i_values, mean_s_10_4 - std_s_10_4, mean_s_10_4 + std_s_10_4, alpha = 0.2)\n",
    "\n",
    "    plt.plot(i_values, mean_s_10_5, alpha = 0.5, label = \"Approximation with $10^5$ samples\")\n",
    "    plt.fill_between(i_values, mean_s_10_5 - std_s_10_5, mean_s_10_5 + std_s_10_5, alpha = 0.2)\n",
    "    \n",
    "    plt.plot(i_values, mean_s_10_6, alpha = 0.5, label = \"Approximation with $10^6$ samples\")\n",
    "    plt.fill_between(i_values, mean_s_10_6 - std_s_10_6, mean_s_10_6 + std_s_10_6, alpha = 0.2)\n",
    "    \n",
    "    plt.plot(i_values, mean_s_10_7, alpha = 0.5, label = \"Approximation with $10^7$ samples\")\n",
    "    plt.fill_between(i_values, mean_s_10_7 - std_s_10_7, mean_s_10_7 + std_s_10_7, alpha = 0.2)\n",
    "    \n",
    "    plt.plot(i_values, mean_s_10_8, alpha = 0.5, label = \"Approximation with $10^8$ samples\")\n",
    "    plt.fill_between(i_values, mean_s_10_8 - std_s_10_8, mean_s_10_8 + std_s_10_8, alpha = 0.2)\n",
    "    \n",
    "    #plt.hlines(1.50659, i_values[0], i_values[-1], label = \"Theoretical Mandelbrot area\", linestyles=\"dashed\")\n",
    "\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mandelbrot area\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"upper right\", fontsize=\"x-small\")\n",
    "    \n",
    "    plt.savefig(title)\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_variance_plotter_appendix(simulation_data = \"monte_carlo_random_sampling\", title = \"line_plot_iterations_random.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variance_box_plot(random_sampling_data, lhs_data, orthogonal_sampling_data, title):\n",
    "    \n",
    "    box_plot_list = [[], [], []]\n",
    "    \n",
    "    if data == np.load(random_sampling_data + \".npz\"):\n",
    "        mean_areas = data['arr_0']*2\n",
    "        sample_variances = data['arr_1']*2 \n",
    "        print(mean_i_100 = mean_areas[1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bounds, u_bounds = (0, 0), (1, 1)\n",
    "nax = 3\n",
    "nsq = nax * nax\n",
    "x_randos, y_randos = random_uniform_orthogonal(nax, l_bounds, u_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_randos, y_randos, s = 0.4, c = \"red\") #TODO?: colourscheme\n",
    "plt.hlines(np.arange(nsq)/nsq, 0, 1, alpha=0.4) # TODO?: replace these with plt.grid with controls\n",
    "plt.vlines(np.arange(nsq)/nsq, 0, 1, alpha=0.4)\n",
    "plt.hlines(np.arange(nax)/nax, 0, 1)\n",
    "plt.vlines(np.arange(nax)/nax, 0, 1)\n",
    "# plt.grid(b = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_methods(n):\n",
    "    nx = ny = n\n",
    "    nsq = n**2\n",
    "    \n",
    "    p_r = np.random.random((nsq, 2))\n",
    "    \n",
    "    sampler = stats.qmc.LatinHypercube(d=2)\n",
    "    lhc = sampler.random(nsq)\n",
    "    \n",
    "    l_bounds, u_bounds = (0, 0), (1, 1)\n",
    "    x_randos, y_randos = random_uniform_orthogonal(n, l_bounds, u_bounds)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(p_r[:, 0], p_r[:, 1], color=\"b\", label=\"Pure random\")\n",
    "    plt.scatter(lhc[:, 0], lhc[:, 1], color=\"r\", label=\"LHC\")\n",
    "    plt.scatter(x_randos, y_randos, color=\"g\", label=\"Orthogonal\")\n",
    "    \n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.hlines(np.arange(nsq)/nsq, 0, 1, alpha = 0.7, color=\"b\")\n",
    "    plt.vlines(np.arange(nsq)/nsq, 0, 1, alpha = 0.7, color=\"b\")\n",
    "    plt.hlines(np.arange(n)/n, 0, 1, color=\"g\")\n",
    "    plt.vlines(np.arange(n)/n, 0, 1, color=\"g\")\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_methods(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_values = np.array([1000, 2000, 3000, 4000, 5000], dtype = np.int64) #TODO: tweak values\n",
    "s_values = np.array([10**i for i in range(4,8)], dtype = np.int64)\n",
    "npz_filename = \"mc_random_fullsave.npz\"\n",
    "if not exists(npz_filename):\n",
    "    re_low, re_high = -2., 1.\n",
    "    im_low, im_high = 0, 1.5\n",
    "\n",
    "    number_of_batches = 50\n",
    "    areas = np.empty((len(i_values),len(s_values), number_of_batches))\n",
    "    mean_areas = np.zeros((len(i_values), len(s_values)))\n",
    "    sample_variances = np.zeros((len(i_values), len(s_values)))\n",
    "\n",
    "    start_time = timer()\n",
    "    for k,i in enumerate(i_values):\n",
    "        # repeat `number_of_batches` times for each s-value\n",
    "        for l,s in enumerate(s_values):\n",
    "            print(f\"Working with i = {i}, s = {s}.\")\n",
    "#             areas_batch = np.zeros(number_of_batches)\n",
    "            for j in range(number_of_batches):\n",
    "                samples = create_batch_uniform(s, re_low, re_high, im_low, im_high)\n",
    "                iter_counts = mandelbrot_mc_default(i, samples)\n",
    "                est_area = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "                areas[k][l][j] = est_area\n",
    "            μ, svar = sample_mean_variance(areas[k][l])\n",
    "            mean_areas[k][l] = μ\n",
    "            sample_variances[k][l] = svar\n",
    "    end_time = timer()\n",
    "    np.savez(npz_filename, mean_areas, sample_variances, areas) \n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    mean_areas = save_data['arr_0']\n",
    "    sample_variances = save_data['arr_1']\n",
    "    areas = save_data['arr_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_random = np.load(\"mc_random_fullsave.npz\")\n",
    "\n",
    "#print(full_random['arr_0']*2)\n",
    "print(full_random['arr_2']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_values = np.array([1000, 2000, 3000, 4000, 5000], dtype = np.int64) #TODO: tweak values\n",
    "s_values = np.array([10**i for i in range(4,8)], dtype = np.int64)\n",
    "def sample_variance_plotter(simulation_data, title):\n",
    "    \"\"\"\"\n",
    "    Plots mean area and standard deviation of the mandelbrot set \n",
    "    against sample sizes simulated with different number of \n",
    "    iterations\n",
    "    \"\"\"\n",
    "    data = np.load(simulation_data + \".npz\")\n",
    "    \n",
    "    mean_areas = data['arr_0']*2\n",
    "    \n",
    "    sample_variances = data['arr_1']*2\n",
    "    \n",
    "    mean_i_1000 = mean_areas[0]\n",
    "    mean_i_2000 = mean_areas[1]\n",
    "    mean_i_3000 = mean_areas[2]\n",
    "    mean_i_4000 = mean_areas[3]\n",
    "    mean_i_5000 = mean_areas[4]\n",
    "\n",
    "\n",
    "    std_i_1000 = np.sqrt(sample_variances[0])\n",
    "    std_i_2000 = np.sqrt(sample_variances[1])\n",
    "    std_i_3000 = np.sqrt(sample_variances[2])\n",
    "    std_i_4000 = np.sqrt(sample_variances[3])\n",
    "    std_i_5000 = np.sqrt(sample_variances[4])\n",
    "\n",
    "\n",
    "    plt.plot(s_values, mean_i_1000, color = \"maroon\", alpha = 0.6, label = \"i = 1000\")\n",
    "    plt.fill_between(s_values, mean_i_1000 - std_i_1000, mean_i_1000 + std_i_1000, alpha = 0.2, color = \"maroon\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_2000, color = \"firebrick\", alpha = 0.6, label = \"i = 2000\")\n",
    "    plt.fill_between(s_values, mean_i_2000 - std_i_2000, mean_i_2000 + std_i_2000, alpha = 0.2, color = \"firebrick\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_3000, color = \"tomato\", alpha = 0.6, label = \"i = 3000\")\n",
    "    plt.fill_between(s_values, mean_i_3000 - std_i_3000, mean_i_3000 + std_i_3000, alpha = 0.2, color = \"tomato\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_4000, color = \"orange\", alpha = 0.6, label = \"i = 4000\")\n",
    "    plt.fill_between(s_values, mean_i_4000 - std_i_4000, mean_i_4000 + std_i_4000, alpha = 0.2, color = \"orange\")\n",
    "\n",
    "    plt.plot(s_values, mean_i_5000, color = \"yellow\", alpha = 0.6, label = \"i = 5000\")\n",
    "    plt.fill_between(s_values, mean_i_5000 - std_i_5000, mean_i_5000 + std_i_5000, alpha = 0.2, color = \"yellow\")\n",
    "\n",
    "    #plt.hlines(1.50659, s_values[0], s_values[-1], label = \"Theoretical Mandelbrot area\", linestyles=\"dashed\")\n",
    "\n",
    "    #plt.ylim(top=max(mean_i_50)+0.06) \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Mandelbrot area\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"upper right\", fontsize=\"x-small\")\n",
    "    \n",
    "    plt.savefig(title)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_variance_plotter(simulation_data = \"mc_random_fullsave\", title = \"line_plot_samples_random.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_random = np.load(\"mc_random_fullsave.npz\")\n",
    "\n",
    "full_random_variances = full_random['arr_1']*2\n",
    "full_random_means = full_random['arr_0']*2\n",
    "\n",
    "print(full_random_means)\n",
    "\n",
    "confidence_intervals(sample_variances = full_random_variances, p = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_random_runs_data = full_random['arr_2']*2\n",
    "# s=10^6\n",
    "i_1000 = full_random_runs_data[0][1]\n",
    "i_2000 = full_random_runs_data[1][1]\n",
    "i_3000 = full_random_runs_data[2][1]\n",
    "i_4000 = full_random_runs_data[3][1]\n",
    "i_5000 = full_random_runs_data[4][1]\n",
    "\n",
    "print(i_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Welch test max iterations and i=1000:\")\n",
    "print(stats.ttest_ind(i_1000, i_5000))\n",
    "print(\"Welch test max iterations and i=2000:\")\n",
    "print(stats.ttest_ind(i_2000, i_5000))\n",
    "print(\"Welch test max iterations and i=3000:\")\n",
    "print(stats.ttest_ind(i_3000, i_5000))\n",
    "print(\"Welch test max iterations and i=4000:\")\n",
    "print(stats.ttest_ind(i_4000, i_5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_1000_s_10_5 = full_random_runs_data[0][1]\n",
    "i_1000_s_10_6 = full_random_runs_data[0][2]\n",
    "i_2000_s_10_5 = full_random_runs_data[1][1]\n",
    "i_2000_s_10_6 = full_random_runs_data[1][2]\n",
    "i_3000_s_10_5 = full_random_runs_data[2][1]\n",
    "i_3000_s_10_6 = full_random_runs_data[2][2]\n",
    "print(i_1000_s_10_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "print(scipy.stats.ttest_1samp(i_1000_s_10_5, 1.5065918849))\n",
    "print(scipy.stats.ttest_1samp(i_1000_s_10_6, 1.5065918849))\n",
    "print(scipy.stats.ttest_1samp(i_2000_s_10_5, 1.5065918849))\n",
    "print(scipy.stats.ttest_1samp(i_2000_s_10_6, 1.5065918849))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Sampling\n",
    "$s = 3000$ gave us a 95% confidence interval of half-width $ < 10^{-3}$. We expect latin hypercube and orthogonal\n",
    "sampling methods to give us a significantly smaller confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def run_batch(s, i, re_low, re_high, im_low, im_high, batch_size, batch_generator):\n",
    "    areas = np.empty(batch_size, dtype = np.float64)\n",
    "    for batch in range(batch_size):\n",
    "        samples = batch_generator(s, re_low, re_high, im_low, im_high)\n",
    "        iter_counts = mandelbrot_mc_default(i, samples)\n",
    "        areas[batch] = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "        return areas\n",
    "    \n",
    "def run_batch_lhc(s, i, re_low, re_high, im_low, im_high, batch_size, seed = None):\n",
    "    areas = np.empty(batch_size, dtype = np.float64)\n",
    "    for batch in range(batch_size):\n",
    "        samples = create_batch_lhc(s, re_low, re_high, im_low, im_high, seed = seed)\n",
    "        iter_counts = mandelbrot_mc_default(i, samples)\n",
    "        areas[batch] = area_from_iter_counts(re_low, re_high, im_low, im_high, iter_counts, i)\n",
    "        return areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10**8\n",
    "i_values = np.array([3000,4000,5000])\n",
    "\n",
    "re_low, re_high = -2, 1\n",
    "im_low, im_high = 0, 1.5\n",
    "\n",
    "number_of_batches = 50\n",
    "\n",
    "npz_filename = \"mc_orthogonal.npz\"\n",
    "areas = []\n",
    "if not exists(npz_filename):\n",
    "    for i in i_values:\n",
    "        print(f\"Working with i = {i}\")\n",
    "        areas.append(run_batch(s, i, re_low, re_high, im_low, im_high, number_of_batches, create_batch_orthogonal))\n",
    "    stats = [sample_mean_variance(batch) for batch in areas]\n",
    "    mean_areas = np.array([stat[0] for stat in stats])\n",
    "    sample_variances = np.array([stat[1] for stat in stats])\n",
    "    areas = np.array(areas)\n",
    "    np.savez(npz_filename, mean_areas, sample_variances, areas)\n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    keys = [f\"arr_{i}\" for i in range(3)]\n",
    "    mean_areas, sample_variances, areas = (save_data[kay] for key in keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latin Hypercube Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10**8\n",
    "i_values = np.array([3000,4000,5000])\n",
    "\n",
    "re_low, re_high = -2, 1\n",
    "im_low, im_high = 0, 1.5\n",
    "\n",
    "number_of_batches = 50\n",
    "\n",
    "npz_filename = \"mc_lhc.npz\"\n",
    "areas = []\n",
    "if not exists(npz_filename):\n",
    "    for i in i_values:\n",
    "        areas.append(run_batch_lhc(s, i, re_low, re_high, im_low, im_high, number_of_batches))\n",
    "    statistics = [sample_mean_variance(batch) for batch in areas]\n",
    "    mean_areas = np.array([stat[0] for stat in statistics])\n",
    "    sample_variances = np.array([stat[1] for stat in statistics])\n",
    "    areas = np.array(areas)\n",
    "    np.savez(npz_filename, mean_areas, sample_variances, areas)\n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    keys = [f\"arr_{i}\" for i in range(3)]\n",
    "    mean_areas, sample_variances, areas = (save_data[kay] for key in keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between PRS, LHC, and OS\n",
    "Fix $i, s$ and vary the number of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "s = 10**5\n",
    "batch_sizes = np.array([10**i for i in range(1, 5)])\n",
    "\n",
    "npz_filename = \"sampling_methods_comparison.npz\"\n",
    "\n",
    "if not exists(npz_filename):\n",
    "    mean_areas = [np.empty(len(batch_sizes)) for _ in range(3)]\n",
    "    sample_variances = [np.empty(len(batch_sizes)) for _ in range(3)]\n",
    "\n",
    "\n",
    "    #pure random sampling\n",
    "    for b in batch_sizes:\n",
    "        areas = run_batch(s, i, re_low, re_high, im_low, im_high, b, create_batch_uniform)\n",
    "        μ, σsq = sample_mean_variance(areas)\n",
    "        mean_areas[0][b], sample_variances[0][b] = μ, σsq\n",
    "\n",
    "    for b in batch_sizes:\n",
    "        areas = run_batch(s, i, re_low, re_high, im_low, im_high, b, create_batch_orthogonal)\n",
    "        μ, σsq = sample_mean_variance(areas)\n",
    "        mean_areas[1][b], sample_variances[1][b] = μ, σsq\n",
    "\n",
    "    for b in batch_sizes:\n",
    "        areas = run_batch_lhc(s, i, re_low, re_high, im_low, im_high, b)\n",
    "        μ, σsq = sample_mean_variance(areas)\n",
    "        mean_areas[2][b], sample_variances[2][b] = μ, σsq\n",
    "    \n",
    "    mean_areas = np.array(mean_areas)\n",
    "    sample_variances = np.array(sample_variances)\n",
    "    \n",
    "    np.savez(npz_filename, mean_areas, sample_variances)\n",
    "else:\n",
    "    save_data = np.load(npz_filename)\n",
    "    indices = (f\"arr_{i}\" for i in range(2))\n",
    "    mean_areas, sample_variances = save_data[i] for i in indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batch_orthogonal?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
